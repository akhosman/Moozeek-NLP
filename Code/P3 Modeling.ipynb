{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc7001b-3b26-4814-b9ac-ef8ce15bb55c",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64ed2b69-ac54-42c6-a05e-5991faa5c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "85064474-0e59-478d-aa03-ca7760f7947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the datasets\n",
    "df=pd.read_csv('df_subs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa020b2a-10ab-428b-851d-b484bbb2bc29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>...</th>\n",
       "      <th>title_lem</th>\n",
       "      <th>selftext_lem_stop</th>\n",
       "      <th>title_lem_stop</th>\n",
       "      <th>title_text_lem_stop</th>\n",
       "      <th>title_len</th>\n",
       "      <th>selftext_len</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>is_ldr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>I really wanted to buy one and I finally saved...</td>\n",
       "      <td>What happened to the etsy shop that sold the h...</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640906654</td>\n",
       "      <td>1.0</td>\n",
       "      <td>artisticphangirl</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>What happened to the etsy shop that sold the h...</td>\n",
       "      <td>I really wanted buy I finally saved But shop D...</td>\n",
       "      <td>What happened etsy shop sold heart necklace</td>\n",
       "      <td>What happened etsy shop sold heart necklace I ...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'comp...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>i was lucky to find it for 56$ at a local reco...</td>\n",
       "      <td>is the standard black nfr vinyl rare?</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640901920</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ambriebat</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>is the standard black nfr vinyl rare</td>\n",
       "      <td>lucky 56 local record shop couple day ago look...</td>\n",
       "      <td>standard black nfr vinyl rare</td>\n",
       "      <td>standard black nfr vinyl rare lucky 56 local r...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>{'neg': 0.044, 'neu': 0.791, 'pos': 0.165, 'co...</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi everyone We are a community-focused music j...</td>\n",
       "      <td>Is Blue Banisters the best folk (or folk-adjac...</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640901006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BLIGATORY</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Is Blue Banisters the best folk or folk adjace...</td>\n",
       "      <td>Hi We community focused music journalism outle...</td>\n",
       "      <td>Is Blue Banisters best folk folk adjacent albu...</td>\n",
       "      <td>Is Blue Banisters best folk folk adjacent albu...</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'comp...</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>what Lana song has this effect on you? VG for ...</td>\n",
       "      <td>I can listen to Video Games every single day a...</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640900733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>throwitawayar</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>I can listen to Video Games every single day a...</td>\n",
       "      <td>Lana song effect VG surreal</td>\n",
       "      <td>I listen Video Games single day I mesmerized b...</td>\n",
       "      <td>I listen Video Games single day I mesmerized b...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>What album out of Lana’s discography would you...</td>\n",
       "      <td>What’s Lana’s Most “Lana Del Rey” Album</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640893285</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Which_Relation_9766</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>What s Lana s Most Lana Del Rey Album</td>\n",
       "      <td>What album Lana discography consider Lana Del Rey</td>\n",
       "      <td>What Lana Most Lana Del Rey Album</td>\n",
       "      <td>What Lana Most Lana Del Rey Album What album L...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit  is_self  is_video  \\\n",
       "0  lanadelrey     True     False   \n",
       "1  lanadelrey     True     False   \n",
       "2  lanadelrey     True     False   \n",
       "3  lanadelrey     True     False   \n",
       "4  lanadelrey     True     False   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I really wanted to buy one and I finally saved...   \n",
       "1  i was lucky to find it for 56$ at a local reco...   \n",
       "2  Hi everyone We are a community-focused music j...   \n",
       "3  what Lana song has this effect on you? VG for ...   \n",
       "4  What album out of Lana’s discography would you...   \n",
       "\n",
       "                                               title subreddit_id  \\\n",
       "0  What happened to the etsy shop that sold the h...     t5_2tegk   \n",
       "1              is the standard black nfr vinyl rare?     t5_2tegk   \n",
       "2  Is Blue Banisters the best folk (or folk-adjac...     t5_2tegk   \n",
       "3  I can listen to Video Games every single day a...     t5_2tegk   \n",
       "4            What’s Lana’s Most “Lana Del Rey” Album     t5_2tegk   \n",
       "\n",
       "   created_utc  upvote_ratio               author  num_comments  ...  \\\n",
       "0   1640906654           1.0     artisticphangirl             0  ...   \n",
       "1   1640901920           1.0            ambriebat             0  ...   \n",
       "2   1640901006           1.0            BLIGATORY             0  ...   \n",
       "3   1640900733           1.0        throwitawayar             0  ...   \n",
       "4   1640893285           1.0  Which_Relation_9766             0  ...   \n",
       "\n",
       "                                           title_lem  \\\n",
       "0  What happened to the etsy shop that sold the h...   \n",
       "1               is the standard black nfr vinyl rare   \n",
       "2  Is Blue Banisters the best folk or folk adjace...   \n",
       "3  I can listen to Video Games every single day a...   \n",
       "4              What s Lana s Most Lana Del Rey Album   \n",
       "\n",
       "                                   selftext_lem_stop  \\\n",
       "0  I really wanted buy I finally saved But shop D...   \n",
       "1  lucky 56 local record shop couple day ago look...   \n",
       "2  Hi We community focused music journalism outle...   \n",
       "3                        Lana song effect VG surreal   \n",
       "4  What album Lana discography consider Lana Del Rey   \n",
       "\n",
       "                                      title_lem_stop  \\\n",
       "0        What happened etsy shop sold heart necklace   \n",
       "1                      standard black nfr vinyl rare   \n",
       "2  Is Blue Banisters best folk folk adjacent albu...   \n",
       "3  I listen Video Games single day I mesmerized b...   \n",
       "4                  What Lana Most Lana Del Rey Album   \n",
       "\n",
       "                                 title_text_lem_stop title_len selftext_len  \\\n",
       "0  What happened etsy shop sold heart necklace I ...         7           11   \n",
       "1  standard black nfr vinyl rare lucky 56 local r...         5           23   \n",
       "2  Is Blue Banisters best folk folk adjacent albu...         9           44   \n",
       "3  I listen Video Games single day I mesmerized b...        10            5   \n",
       "4  What Lana Most Lana Del Rey Album What album L...         7            8   \n",
       "\n",
       "                                              scores  compound  \\\n",
       "0  {'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'comp...    0.2263   \n",
       "1  {'neg': 0.044, 'neu': 0.791, 'pos': 0.165, 'co...    0.5859   \n",
       "2  {'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'comp...    0.9531   \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   \n",
       "\n",
       "   compound_score is_ldr  \n",
       "0        positive      1  \n",
       "1        positive      1  \n",
       "2        positive      1  \n",
       "3         neutral      1  \n",
       "4         neutral      1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972e100-80a1-43dd-8b8b-7b3690a45553",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10064e7c-a6aa-407c-be49-b56d4b186dee",
   "metadata": {},
   "source": [
    "To compare our models with a baseline model, we will first create the baseline model using the normalized value of our response (y) or in other words the percentage of y within our target. This will be the representative of the simplest model whereby assigning a post randomly will yield ~60% chance of correct classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9f003d6a-e2bd-49d7-83c7-7af27166a829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metallica     561\n",
       "lanadelrey    521\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fce9c06-d4a2-4d17-a4c3-91f286f98091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metallica     0.518484\n",
       "lanadelrey    0.481516\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "87d8dc31-a735-4465-bb55-3a3ab67e3d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['title_text_lem_stop']\n",
    "y = df['is_ldr']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876cd8f3-1a9e-42cf-baa5-5d35370b4ef2",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "490a2315-f58a-4a76-8b4f-16c2344b441d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into train and test data. We will stratify during the split to ensure that the train and test sets \n",
    "# contains the same percentage of samplesto avoid imbalanced classes.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,\n",
    "                                                 random_state=42,\n",
    "                                                 stratify=y) # account for slight class unbalanced "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a1d54-22ea-4718-b139-206f333f64ee",
   "metadata": {},
   "source": [
    "### Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c342edc9-fab3-492d-8009-0dac529a6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizer and Model imports:\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier,GradientBoostingClassifier, AdaBoostClassifier,VotingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report, plot_roc_curve, roc_auc_score,accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48d9c2cb-d82f-403b-9395-7d2325ba3486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom stop word list identified\n",
    "custom_stopword=['lanadelrey','metallica','ha','lol','wa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "10a91cdc-e9e7-4474-aacb-6d5211cad97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=custom_stopword)\n",
    "tvec = TfidfVectorizer(stop_words=custom_stopword)\n",
    "hv= HashingVectorizer(stop_words=custom_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "547951a8-5339-45df-9064-5966f354daee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate vectorizers\n",
    "vectorizers = {'cvec': cv,\n",
    "               'tvec': tvec,\n",
    "               'hv': hv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08708e23-35f8-48a3-b14d-85f09a6d5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instiantiate models\n",
    "models = {'lr': LogisticRegression(max_iter=1_000, random_state=42),\n",
    "          'rf': RandomForestClassifier(random_state=42),\n",
    "          'gb': GradientBoostingClassifier(random_state=42),\n",
    "          'et': ExtraTreesClassifier(random_state=42),\n",
    "          'ada': AdaBoostClassifier(random_state=42),\n",
    "          'nb': MultinomialNB(),\n",
    "          'svc': SVC(random_state=42)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f7e731fc-a401-4805-bc59-1e46fb512d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run model -- input vectorizer and model\n",
    "def model_run(vec, mod, vec_params={}, mod_params={}, grid_search=False):\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "            (vec, vectorizers[vec]),\n",
    "            (mod, models[mod])\n",
    "            ])\n",
    "    \n",
    "    if grid_search:\n",
    "        gs = GridSearchCV(pipe, param_grid = {**vec_params, **mod_params}, verbose=3, n_jobs=-1)\n",
    "        gs.fit(X_train, y_train)\n",
    "        pipe = gs\n",
    "        \n",
    "    else:\n",
    "        pipe.fit(X_train, y_train)\n",
    "    \n",
    "    # Retrieve metrics\n",
    "    results['model'] = mod\n",
    "    results['vectorizer'] = vec\n",
    "    results['train'] = pipe.score(X_train, y_train)\n",
    "    results['test'] = pipe.score(X_test, y_test)\n",
    "    predictions = pipe.predict(X_test)\n",
    "    results['roc'] = roc_auc_score(y_test, predictions)\n",
    "    results['precision'] = precision_score(y_test, predictions)\n",
    "    results['recall'] = recall_score(y_test, predictions)\n",
    "    results['f_score'] = f1_score(y_test, predictions)\n",
    "    \n",
    "    if grid_search:\n",
    "        tuning_list.append(results)\n",
    "        print('PARAMS')\n",
    "        display(pipe.best_params_)\n",
    "        \n",
    "    else:\n",
    "        eval_list.append(results)\n",
    "    \n",
    "    print('METRICS')\n",
    "    display(results)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    \n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613ffe3-6b6b-4034-8001-c9c73f601e55",
   "metadata": {},
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58fa318a-edda-4d7b-8309-7bc4c82c61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list to store model testing results\n",
    "eval_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1dc1dc-673b-40c4-918d-dd0e74098e37",
   "metadata": {},
   "source": [
    "### Benchmark Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae896d-795f-43a6-9ab5-52f5a39243bf",
   "metadata": {},
   "source": [
    "Since there are significant differences on the numerical features of the posts extracted from both subreddits (e.g. title/post length and number of comments), we should also test out a model with only these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0e306e05-c393-4361-a33f-4c8cc5b132a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>...</th>\n",
       "      <th>title_lem</th>\n",
       "      <th>selftext_lem_stop</th>\n",
       "      <th>title_lem_stop</th>\n",
       "      <th>title_text_lem_stop</th>\n",
       "      <th>title_len</th>\n",
       "      <th>selftext_len</th>\n",
       "      <th>scores</th>\n",
       "      <th>compound</th>\n",
       "      <th>compound_score</th>\n",
       "      <th>is_ldr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>I really wanted to buy one and I finally saved...</td>\n",
       "      <td>What happened to the etsy shop that sold the h...</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640906654</td>\n",
       "      <td>1.0</td>\n",
       "      <td>artisticphangirl</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>What happened to the etsy shop that sold the h...</td>\n",
       "      <td>I really wanted buy I finally saved But shop D...</td>\n",
       "      <td>What happened etsy shop sold heart necklace</td>\n",
       "      <td>What happened etsy shop sold heart necklace I ...</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'comp...</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>i was lucky to find it for 56$ at a local reco...</td>\n",
       "      <td>is the standard black nfr vinyl rare?</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640901920</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ambriebat</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>is the standard black nfr vinyl rare</td>\n",
       "      <td>lucky 56 local record shop couple day ago look...</td>\n",
       "      <td>standard black nfr vinyl rare</td>\n",
       "      <td>standard black nfr vinyl rare lucky 56 local r...</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>{'neg': 0.044, 'neu': 0.791, 'pos': 0.165, 'co...</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>Hi everyone We are a community-focused music j...</td>\n",
       "      <td>Is Blue Banisters the best folk (or folk-adjac...</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640901006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BLIGATORY</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Is Blue Banisters the best folk or folk adjace...</td>\n",
       "      <td>Hi We community focused music journalism outle...</td>\n",
       "      <td>Is Blue Banisters best folk folk adjacent albu...</td>\n",
       "      <td>Is Blue Banisters best folk folk adjacent albu...</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'comp...</td>\n",
       "      <td>0.9531</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>what Lana song has this effect on you? VG for ...</td>\n",
       "      <td>I can listen to Video Games every single day a...</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640900733</td>\n",
       "      <td>1.0</td>\n",
       "      <td>throwitawayar</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>I can listen to Video Games every single day a...</td>\n",
       "      <td>Lana song effect VG surreal</td>\n",
       "      <td>I listen Video Games single day I mesmerized b...</td>\n",
       "      <td>I listen Video Games single day I mesmerized b...</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lanadelrey</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>What album out of Lana’s discography would you...</td>\n",
       "      <td>What’s Lana’s Most “Lana Del Rey” Album</td>\n",
       "      <td>t5_2tegk</td>\n",
       "      <td>1640893285</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Which_Relation_9766</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>What s Lana s Most Lana Del Rey Album</td>\n",
       "      <td>What album Lana discography consider Lana Del Rey</td>\n",
       "      <td>What Lana Most Lana Del Rey Album</td>\n",
       "      <td>What Lana Most Lana Del Rey Album What album L...</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit  is_self  is_video  \\\n",
       "0  lanadelrey     True     False   \n",
       "1  lanadelrey     True     False   \n",
       "2  lanadelrey     True     False   \n",
       "3  lanadelrey     True     False   \n",
       "4  lanadelrey     True     False   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  I really wanted to buy one and I finally saved...   \n",
       "1  i was lucky to find it for 56$ at a local reco...   \n",
       "2  Hi everyone We are a community-focused music j...   \n",
       "3  what Lana song has this effect on you? VG for ...   \n",
       "4  What album out of Lana’s discography would you...   \n",
       "\n",
       "                                               title subreddit_id  \\\n",
       "0  What happened to the etsy shop that sold the h...     t5_2tegk   \n",
       "1              is the standard black nfr vinyl rare?     t5_2tegk   \n",
       "2  Is Blue Banisters the best folk (or folk-adjac...     t5_2tegk   \n",
       "3  I can listen to Video Games every single day a...     t5_2tegk   \n",
       "4            What’s Lana’s Most “Lana Del Rey” Album     t5_2tegk   \n",
       "\n",
       "   created_utc  upvote_ratio               author  num_comments  ...  \\\n",
       "0   1640906654           1.0     artisticphangirl             0  ...   \n",
       "1   1640901920           1.0            ambriebat             0  ...   \n",
       "2   1640901006           1.0            BLIGATORY             0  ...   \n",
       "3   1640900733           1.0        throwitawayar             0  ...   \n",
       "4   1640893285           1.0  Which_Relation_9766             0  ...   \n",
       "\n",
       "                                           title_lem  \\\n",
       "0  What happened to the etsy shop that sold the h...   \n",
       "1               is the standard black nfr vinyl rare   \n",
       "2  Is Blue Banisters the best folk or folk adjace...   \n",
       "3  I can listen to Video Games every single day a...   \n",
       "4              What s Lana s Most Lana Del Rey Album   \n",
       "\n",
       "                                   selftext_lem_stop  \\\n",
       "0  I really wanted buy I finally saved But shop D...   \n",
       "1  lucky 56 local record shop couple day ago look...   \n",
       "2  Hi We community focused music journalism outle...   \n",
       "3                        Lana song effect VG surreal   \n",
       "4  What album Lana discography consider Lana Del Rey   \n",
       "\n",
       "                                      title_lem_stop  \\\n",
       "0        What happened etsy shop sold heart necklace   \n",
       "1                      standard black nfr vinyl rare   \n",
       "2  Is Blue Banisters best folk folk adjacent albu...   \n",
       "3  I listen Video Games single day I mesmerized b...   \n",
       "4                  What Lana Most Lana Del Rey Album   \n",
       "\n",
       "                                 title_text_lem_stop title_len selftext_len  \\\n",
       "0  What happened etsy shop sold heart necklace I ...         7           11   \n",
       "1  standard black nfr vinyl rare lucky 56 local r...         5           23   \n",
       "2  Is Blue Banisters best folk folk adjacent albu...         9           44   \n",
       "3  I listen Video Games single day I mesmerized b...        10            5   \n",
       "4  What Lana Most Lana Del Rey Album What album L...         7            8   \n",
       "\n",
       "                                              scores  compound  \\\n",
       "0  {'neg': 0.0, 'neu': 0.888, 'pos': 0.112, 'comp...    0.2263   \n",
       "1  {'neg': 0.044, 'neu': 0.791, 'pos': 0.165, 'co...    0.5859   \n",
       "2  {'neg': 0.0, 'neu': 0.736, 'pos': 0.264, 'comp...    0.9531   \n",
       "3  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   \n",
       "4  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...    0.0000   \n",
       "\n",
       "   compound_score is_ldr  \n",
       "0        positive      1  \n",
       "1        positive      1  \n",
       "2        positive      1  \n",
       "3         neutral      1  \n",
       "4         neutral      1  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "28810b05-50f9-4fe9-81fc-7cc9669ed4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_len'] = len(df['title_lem_stop'])\n",
    "df['selftext_len'] = len(df['selftext_lem_stop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "047c11f1-d0fc-4039-a441-f3a10150e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm stands for benchmark\n",
    "X_bm =df[['upvote_ratio', 'num_comments', 'title_len', 'selftext_len', 'compound']]\n",
    "y_bm = df['is_ldr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55a83253-aa1c-4d35-be38-30ddec44d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split our data into train and test data\n",
    "X_bm_train, X_bm_test, y_bm_train, y_bm_test = train_test_split(X_bm, y_bm, test_size=0.3, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4d2bf866-a350-4165-8298-ee3b271388b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_bm_train, y_bm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6b52259a-81e9-42c8-bc83-6cb90d49d19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5759577278731837"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_bm_train, y_bm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1f58981e-7a91-4962-9d8c-1b71dce2bc79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.556923076923077"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.score(X_bm_test, y_bm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c9431ebb-f58a-4007-ac64-1271084a56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm1_pred = logreg.predict(X_bm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3355098f-a41c-4f5d-9b8c-ce6e56b730d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 91\n",
      "False Positives: 78\n",
      "False Negatives: 66\n",
      "True Positives: 90\n"
     ]
    }
   ],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_bm_test, bm1_pred).ravel()\n",
    "\n",
    "print(\"True Negatives: %s\" % tn)\n",
    "print(\"False Positives: %s\" % fp)\n",
    "print(\"False Negatives: %s\" % fn)\n",
    "print(\"True Positives: %s\" % tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884e137-1ed1-49b5-b61b-5a4dd57dce89",
   "metadata": {},
   "source": [
    "True Positives are r/lanadelrey posts that were correctly classified by our model. True Negatives are r/metallica posts that were correctly classified by our model. We can see that the model still incorrectly classifies about 78 posts from lanadelrey subreddit and 60 from metallica subreddit which is not ideal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6f1655fc-d734-4573-8540-042123b8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.54      0.56       169\n",
      "           1       0.54      0.58      0.56       156\n",
      "\n",
      "    accuracy                           0.56       325\n",
      "   macro avg       0.56      0.56      0.56       325\n",
      "weighted avg       0.56      0.56      0.56       325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_bm_test, bm1_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f3db9-59c6-4555-ab1f-45761c849f4e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ac2377c-3fa5-4105-9323-b7e86954f5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.9975339087546239,\n",
       " 'test': 0.9114391143911439,\n",
       " 'roc': 0.9109929078014184,\n",
       " 'precision': 0.9140625,\n",
       " 'recall': 0.9,\n",
       " 'f_score': 0.9069767441860466}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 130\n",
      "False Positives: 11\n",
      "False Negatives: 13\n",
      "True Positives: 117\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with CountVectorizer\n",
    "cvec_lr = model_run('cvec', 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e155552-6a4d-4b9b-899b-e35ee5bf9fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.9901356350184957,\n",
       " 'test': 0.9446494464944649,\n",
       " 'roc': 0.9444080741953083,\n",
       " 'precision': 0.9457364341085271,\n",
       " 'recall': 0.9384615384615385,\n",
       " 'f_score': 0.942084942084942}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 134\n",
      "False Positives: 7\n",
      "False Negatives: 8\n",
      "True Positives: 122\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with TfdifVectorizer\n",
    "tvec_lr = model_run('tvec', 'lr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad57fa1a-8e6c-4967-928a-001edca95e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'hv',\n",
       " 'train': 0.9852034525277436,\n",
       " 'test': 0.9261992619926199,\n",
       " 'roc': 0.9248772504091654,\n",
       " 'precision': 0.9508196721311475,\n",
       " 'recall': 0.8923076923076924,\n",
       " 'f_score': 0.9206349206349206}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 135\n",
      "False Positives: 6\n",
      "False Negatives: 14\n",
      "True Positives: 116\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with TfdifVectorizer\n",
    "tvec_lr = model_run('hv', 'lr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac848f-1d9b-4e5b-9821-efc272720c57",
   "metadata": {},
   "source": [
    "HashingVectorizer() was also experimented as it has low memory requirement by storing tokens as strings but because this we can no longer retrieve the features after vectorizing. Besides, we decided to drop using this in subsequent models due to weaker scores compared to the other two vectorizers.\n",
    "\n",
    "Comparing TVEC,CVEC and HashVectorizer, clearly tvec yields a higher R2 score and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586c096a-313c-4b77-8417-bf1b1a0d1ec1",
   "metadata": {},
   "source": [
    "### Random Forest / Extra Tree Classifier with CVEC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f19ade-0694-4f78-8966-4d6bea484a92",
   "metadata": {},
   "source": [
    "Random forest is a tree-based machine learning algorithm that leverages the power of multiple decision trees for making decisions. Each node in the decision tree works on a random subset of features to calculate the output which is aggregated to form the final output.\n",
    "\n",
    "The Extra Trees classifier works similar to this, but incorporates bootstrap aggregation (or random sampling with replacement) in order to reduce variance and help with overfitting. In general, both methods provided worse results relative to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ec39ecb6-7cab-4aa6-aab7-114bc0838a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'rf',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 1.0,\n",
       " 'test': 0.9261992619926199,\n",
       " 'roc': 0.9281778505182761,\n",
       " 'precision': 0.8819444444444444,\n",
       " 'recall': 0.9769230769230769,\n",
       " 'f_score': 0.927007299270073}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 124\n",
      "False Positives: 17\n",
      "False Negatives: 3\n",
      "True Positives: 127\n"
     ]
    }
   ],
   "source": [
    "cvec_rf = model_run('cvec', 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "cc7f836a-4dc0-4745-ac53-ada455466660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'rf',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 1.0,\n",
       " 'test': 0.9188191881918819,\n",
       " 'roc': 0.9207855973813421,\n",
       " 'precision': 0.875,\n",
       " 'recall': 0.9692307692307692,\n",
       " 'f_score': 0.9197080291970802}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 123\n",
      "False Positives: 18\n",
      "False Negatives: 4\n",
      "True Positives: 126\n"
     ]
    }
   ],
   "source": [
    "tvec_rf = model_run('tvec', 'rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "db44bb2b-d630-403b-a3c2-1e488cf6268e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'et',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 1.0,\n",
       " 'test': 0.9372693726937269,\n",
       " 'roc': 0.9385160938352428,\n",
       " 'precision': 0.9064748201438849,\n",
       " 'recall': 0.9692307692307692,\n",
       " 'f_score': 0.9368029739776952}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 128\n",
      "False Positives: 13\n",
      "False Negatives: 4\n",
      "True Positives: 126\n"
     ]
    }
   ],
   "source": [
    "cvec_et = model_run('cvec', 'et')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "36ca286b-856e-4257-bdb5-5750fc422057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'et',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 1.0,\n",
       " 'test': 0.9188191881918819,\n",
       " 'roc': 0.9210856519367159,\n",
       " 'precision': 0.8698630136986302,\n",
       " 'recall': 0.9769230769230769,\n",
       " 'f_score': 0.9202898550724639}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 122\n",
      "False Positives: 19\n",
      "False Negatives: 3\n",
      "True Positives: 127\n"
     ]
    }
   ],
   "source": [
    "tvec_et = model_run('tvec', 'et')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff37d5-3270-4061-87a1-f90552c7db51",
   "metadata": {},
   "source": [
    "Both random forest and extra trees classifier did not score that well compared to logistic regression with tvec. We would need to explore further on how to get a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efab748-98dc-4a20-ab24-35c71fa0908b",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb30622-c4b5-45f8-8a07-ef5470aecb67",
   "metadata": {},
   "source": [
    "Naive Bayes classifiers works off Bayes' theroem, which describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "\n",
    "In this case we will be using Multinomial Naive Bayes that looks at the frequency of the words present in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f5e483ae-e4bb-40fe-a363-d2e3bc24d050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'nb',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.9901356350184957,\n",
       " 'test': 0.955719557195572,\n",
       " 'roc': 0.9562465902891435,\n",
       " 'precision': 0.9402985074626866,\n",
       " 'recall': 0.9692307692307692,\n",
       " 'f_score': 0.9545454545454547}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 133\n",
      "False Positives: 8\n",
      "False Negatives: 4\n",
      "True Positives: 126\n"
     ]
    }
   ],
   "source": [
    "cvec_nb = model_run('cvec', 'nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c29755db-cb2c-4bde-8c95-782c4d445c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'nb',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.9889025893958077,\n",
       " 'test': 0.955719557195572,\n",
       " 'roc': 0.9562465902891435,\n",
       " 'precision': 0.9402985074626866,\n",
       " 'recall': 0.9692307692307692,\n",
       " 'f_score': 0.9545454545454547}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 133\n",
      "False Positives: 8\n",
      "False Negatives: 4\n",
      "True Positives: 126\n"
     ]
    }
   ],
   "source": [
    "tvec_nb = model_run('tvec', 'nb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cf39ac-214d-4a04-ae16-7c71ef9c374e",
   "metadata": {},
   "source": [
    "The Multinomial Naive Bayes classifier with count vectorizer has an extremely high recall and f-score which is great at predicting r/lanadelrey posts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed923ea8-fd3e-4140-a2a5-9d315ba84c14",
   "metadata": {},
   "source": [
    "### Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac72c35-65c5-4556-9878-d4e6babd5d8f",
   "metadata": {},
   "source": [
    "Support vector machine classifer separates data points using a hyperplane with the largest amount of margin and classifies them accordingly. In other words, the algorithm determines the best decision boundary between vectors that belong to a given group (or category) and vectors that do not belong to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6f12f0eb-d356-4dd3-9110-387399c5c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'svc',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.9605425400739828,\n",
       " 'test': 0.8782287822878229,\n",
       " 'roc': 0.8739770867430441,\n",
       " 'precision': 0.970873786407767,\n",
       " 'recall': 0.7692307692307693,\n",
       " 'f_score': 0.8583690987124465}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 138\n",
      "False Positives: 3\n",
      "False Negatives: 30\n",
      "True Positives: 100\n"
     ]
    }
   ],
   "source": [
    "cvec_svc = model_run('cvec', 'svc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad18100-8742-402c-ba82-b808bb1748f1",
   "metadata": {},
   "source": [
    "tvec_svc = model_run('tvec', 'svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2d8071ee-fd92-425f-b4de-f06b4d9eaeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "10b2e9d2-49bd-46d2-b4cb-40d43ce5d7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>roc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.990136</td>\n",
       "      <td>0.955720</td>\n",
       "      <td>0.956247</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.988903</td>\n",
       "      <td>0.955720</td>\n",
       "      <td>0.956247</td>\n",
       "      <td>0.940299</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.954545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lr</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.990136</td>\n",
       "      <td>0.944649</td>\n",
       "      <td>0.944408</td>\n",
       "      <td>0.945736</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.942085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>et</td>\n",
       "      <td>cvec</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.937269</td>\n",
       "      <td>0.938516</td>\n",
       "      <td>0.906475</td>\n",
       "      <td>0.969231</td>\n",
       "      <td>0.936803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lr</td>\n",
       "      <td>hv</td>\n",
       "      <td>0.985203</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.924877</td>\n",
       "      <td>0.950820</td>\n",
       "      <td>0.892308</td>\n",
       "      <td>0.920635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rf</td>\n",
       "      <td>cvec</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926199</td>\n",
       "      <td>0.928178</td>\n",
       "      <td>0.881944</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.927007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model vectorizer     train      test       roc  precision    recall  \\\n",
       "0    nb       cvec  0.990136  0.955720  0.956247   0.940299  0.969231   \n",
       "1    nb       tvec  0.988903  0.955720  0.956247   0.940299  0.969231   \n",
       "2    lr       tvec  0.990136  0.944649  0.944408   0.945736  0.938462   \n",
       "3    et       cvec  1.000000  0.937269  0.938516   0.906475  0.969231   \n",
       "4    lr         hv  0.985203  0.926199  0.924877   0.950820  0.892308   \n",
       "5    rf       cvec  1.000000  0.926199  0.928178   0.881944  0.976923   \n",
       "\n",
       "    f_score  \n",
       "0  0.954545  \n",
       "1  0.954545  \n",
       "2  0.942085  \n",
       "3  0.936803  \n",
       "4  0.920635  \n",
       "5  0.927007  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top results (Accuracy >= 0.790)\n",
    "eval_df.sort_values(by='test', ascending=False).reset_index(drop=True).head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e23311-cb0d-4e99-8b16-d1dd406a82b0",
   "metadata": {},
   "source": [
    "we can see that both naive bayes with cvec & tvec rank did better in terms of test score, roc score, recall and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7631d8-9b3d-472d-928a-d493a69d888c",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a4f4896a-81fe-4e90-9789-0ef94ef219d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate list to store tuning results\n",
    "tuning_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "94d0aded-fb33-4169-a2f8-87f21ab9e761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analyzer': 'word',\n",
       " 'binary': False,\n",
       " 'decode_error': 'strict',\n",
       " 'dtype': numpy.int64,\n",
       " 'encoding': 'utf-8',\n",
       " 'input': 'content',\n",
       " 'lowercase': True,\n",
       " 'max_df': 1.0,\n",
       " 'max_features': None,\n",
       " 'min_df': 1,\n",
       " 'ngram_range': (1, 1),\n",
       " 'preprocessor': None,\n",
       " 'stop_words': ['lanadelrey', 'metallica', 'ha', 'lol', 'wa'],\n",
       " 'strip_accents': None,\n",
       " 'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'tokenizer': None,\n",
       " 'vocabulary': None}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7d0b501-1e30-4b88-9786-81ee606d4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec_params = {\n",
    "    # Setting a limit of n-number of features included/vocab size\n",
    "    'cvec__max_features': [None, 12_000],\n",
    "\n",
    "    # Setting a minimum number of times the word/token has to appear in n-documents\n",
    "    'cvec__min_df':[1, 2, 3],\n",
    "    \n",
    "    # Setting an upper threshold/max percentage of n% of documents from corpus \n",
    "    'cvec__max_df': [0.1, 0.2, 1],\n",
    "    \n",
    "    # Testing with bigrams and trigrams\n",
    "    'cvec__ngram_range':[(1,1), (1,2)],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "325301b6-a7d0-48ed-a3d9-97d10e35dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec_params = {\n",
    "    'tvec__max_features': [None],\n",
    "    'tvec__min_df':[3, 4, 5],\n",
    "    'tvec__max_df': [0.2, 0.3, 0.4],\n",
    "    'tvec__stop_words': ['english'],\n",
    "    'tvec__ngram_range':[(1,1), (1,2)]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e7841-09ae-488c-ae86-6cd06c758e58",
   "metadata": {},
   "source": [
    "### Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ef97edd8-4c46-4b04-9521-170061ed2699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultinomialNB().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76de7d3b-a499-487f-b4da-c4c352668367",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_params = {\n",
    "    # Trying different types of regularization\n",
    "    'lr__penalty':['l2','l1'],\n",
    "\n",
    "     # Trying different alphas of: 10, 1, 0.1 (C = 1/alpha)\n",
    "    'lr__C':[0.1, 1, 10],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e94e18a5-6042-43ec-a6e8-4e06114f257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_params = {\n",
    "    'nb__fit_prior': [True, False],\n",
    "    'nb__alpha': [0.8, 0.9, 1.0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8d86bc7-2d81-48f2-bc60-b606ba6ec060",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_params = {\n",
    "    'svc__C':[0.1, 1, 10],\n",
    "    'svc__gamma':[0.01, 0.1, 0.3], \n",
    "    'svc__kernel':['linear','rbf'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b70762-5228-41f1-aa85-f2ad97862bef",
   "metadata": {},
   "source": [
    "### Logistic Regression with CVEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "71250dc3-51a8-4ee0-ac00-29af8fe18693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "PARAMS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'lr__C': 0.1,\n",
       " 'lr__penalty': 'l2'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.9963008631319359,\n",
       " 'test': 0.9003690036900369,\n",
       " 'roc': 0.9012547735951992,\n",
       " 'precision': 0.8759124087591241,\n",
       " 'recall': 0.9230769230769231,\n",
       " 'f_score': 0.8988764044943821}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 124\n",
      "False Positives: 17\n",
      "False Negatives: 10\n",
      "True Positives: 120\n"
     ]
    }
   ],
   "source": [
    "# Always stop_words & never trigrams (best_results without model tuning)\n",
    "cvec_lr_gs = model_run('cvec', 'lr', vec_params=cvec_params, mod_params=lr_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75704fd7-544e-49bb-b976-a462a36b96a6",
   "metadata": {},
   "source": [
    "### Logistic Regression with TVEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fc52f882-9736-4d61-bf31-d2500deee2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "PARAMS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'lr__C': 1,\n",
       " 'lr__penalty': 'l2',\n",
       " 'tvec__max_df': 0.3,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 3,\n",
       " 'tvec__ngram_range': (1, 2),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'lr',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.9926017262638718,\n",
       " 'test': 0.959409594095941,\n",
       " 'roc': 0.9585924713584287,\n",
       " 'precision': 0.976,\n",
       " 'recall': 0.9384615384615385,\n",
       " 'f_score': 0.9568627450980391}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 138\n",
      "False Positives: 3\n",
      "False Negatives: 8\n",
      "True Positives: 122\n"
     ]
    }
   ],
   "source": [
    "tvec_lr_gs = model_run('tvec', 'lr', vec_params=tvec_params, mod_params=lr_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16183a8-ffdf-436e-b82c-abed93b13f6b",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes with CVEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "77a1b0d3-692c-402f-9997-a1796ce63e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
      "PARAMS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cvec__max_df': 0.2,\n",
       " 'cvec__max_features': None,\n",
       " 'cvec__min_df': 1,\n",
       " 'cvec__ngram_range': (1, 2),\n",
       " 'nb__alpha': 0.9,\n",
       " 'nb__fit_prior': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'nb',\n",
       " 'vectorizer': 'cvec',\n",
       " 'train': 0.9963008631319359,\n",
       " 'test': 0.9520295202952029,\n",
       " 'roc': 0.9524004364429897,\n",
       " 'precision': 0.9398496240601504,\n",
       " 'recall': 0.9615384615384616,\n",
       " 'f_score': 0.9505703422053231}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 133\n",
      "False Positives: 8\n",
      "False Negatives: 5\n",
      "True Positives: 125\n"
     ]
    }
   ],
   "source": [
    "cvec_nb_gs = model_run('cvec', 'nb', vec_params=cvec_params, mod_params=nb_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb0b808-b273-4ed8-80ab-77b42072242f",
   "metadata": {},
   "source": [
    "### Support Machine Classifier with TVEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "17500ada-412e-4289-81c9-6b04ac1f9f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "PARAMS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'svc__C': 10,\n",
       " 'svc__gamma': 0.01,\n",
       " 'svc__kernel': 'rbf',\n",
       " 'tvec__max_df': 0.4,\n",
       " 'tvec__max_features': None,\n",
       " 'tvec__min_df': 3,\n",
       " 'tvec__ngram_range': (1, 1),\n",
       " 'tvec__stop_words': 'english'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METRICS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model': 'svc',\n",
       " 'vectorizer': 'tvec',\n",
       " 'train': 0.9728729963008631,\n",
       " 'test': 0.940959409594096,\n",
       " 'roc': 0.9405619203491545,\n",
       " 'precision': 0.9453125,\n",
       " 'recall': 0.9307692307692308,\n",
       " 'f_score': 0.937984496124031}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Negatives: 134\n",
      "False Positives: 7\n",
      "False Negatives: 9\n",
      "True Positives: 121\n"
     ]
    }
   ],
   "source": [
    "tvec_svc_gs = model_run('tvec', 'svc', vec_params=tvec_params, mod_params=svc_params, grid_search=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea66e716-3de0-460e-96fb-3c27f7f60333",
   "metadata": {},
   "source": [
    "### Final Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3c0c23c9-9c88-443a-8ac9-b92c7b0b9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_df = pd.DataFrame(tuning_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "409a2b12-0173-454c-97b0-770cde019aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>vectorizer</th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>roc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.992602</td>\n",
       "      <td>0.959410</td>\n",
       "      <td>0.958592</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.956863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nb</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.996301</td>\n",
       "      <td>0.952030</td>\n",
       "      <td>0.952400</td>\n",
       "      <td>0.939850</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.950570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svc</td>\n",
       "      <td>tvec</td>\n",
       "      <td>0.972873</td>\n",
       "      <td>0.940959</td>\n",
       "      <td>0.940562</td>\n",
       "      <td>0.945312</td>\n",
       "      <td>0.930769</td>\n",
       "      <td>0.937984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lr</td>\n",
       "      <td>cvec</td>\n",
       "      <td>0.996301</td>\n",
       "      <td>0.900369</td>\n",
       "      <td>0.901255</td>\n",
       "      <td>0.875912</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.898876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model vectorizer     train      test       roc  precision    recall  \\\n",
       "0    lr       tvec  0.992602  0.959410  0.958592   0.976000  0.938462   \n",
       "1    nb       cvec  0.996301  0.952030  0.952400   0.939850  0.961538   \n",
       "2   svc       tvec  0.972873  0.940959  0.940562   0.945312  0.930769   \n",
       "3    lr       cvec  0.996301  0.900369  0.901255   0.875912  0.923077   \n",
       "\n",
       "    f_score  \n",
       "0  0.956863  \n",
       "1  0.950570  \n",
       "2  0.937984  \n",
       "3  0.898876  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_df.sort_values(by=['test', 'roc'], ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a48a7ec-463c-4c50-8fee-6e23d13179ad",
   "metadata": {},
   "source": [
    "From the table above, linear regression with TfidfVectorizer returned the highest R2 accuracy of 0.96 in gridsearch while 0.96 w/o gridsearch even on the default params. This is because grid search creates subsamples of the data repeatedly. GridSearch is used for selecting a combination of hyperparameters, performance estimation has not yet happened. The only comparison we could be making is between the parameter combinations within the CV itself.\n",
    "\n",
    "In other words, our model is able accurately predict about 96% of the test data based on our text features. The model also has the best AUC-ROC score of 0.96. We can interpret this metric as proof that that this model is the best at distinguishing between classes. The model does particularly very well in terms of recall (0.94), with only 8 false negatives (predicted r/metallica but actually belong to r/lanadelrey posts which is potentially a loss of Lana Del Rey fans).\n",
    "\n",
    "In summary, our final model:\n",
    "\n",
    "- The model uses Tfid Vectorization \n",
    "- ignores terms that that appear in more than 30% of posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b820f1-8583-4c8d-afce-deaf58360288",
   "metadata": {},
   "source": [
    "### Conclusion and Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f05c11-a5bc-40ec-a70d-ec43069d71e1",
   "metadata": {},
   "source": [
    "Conclusion & Recommendations\n",
    "In conclusion, the model we chosed to better classify the posts for r/lanadelrey and r/metallica is the linear regression model with tfid vectorizer due to its higher r2 score, recall and f-1 score as a whole.\n",
    "\n",
    "Besides maximizing the funds and manpower to classify posts from two different subreddits based on their title and selftext, there are a number of other possible applications for this model.\n",
    "\n",
    "By looking at the probabilities associated with each post, marketing teams can better appeal to the potential listeners when they are promoting to the LDR fans or Metallica fans. In fact, this can also be useful as words can be trending with time but withi this model we can accurately ride the trend with words that have high probability of being classified as artist names and song names.\n",
    "\n",
    "The sentiment analysis we implemented can also determine the mood of the potential clients which either is a positive attraction or vice versa.\n",
    "\n",
    "The recommendation we would propose at this point of time for the Moozeek is that we can focus on are as follows:\n",
    "\n",
    "- When someone wants to listen to a certain genre, they do not want to be interupted with a sudden change of tone/tune. Classifying keywords should be a priority to categorize the correct artist to the correct genre. Also, makes a run for a great listening experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7401b254-f9e3-42cf-8c25-6c7817168b1f",
   "metadata": {},
   "source": [
    "- Dig further into keyword predicitions for moods in regards to genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07197145-553b-49bb-9ef7-809c317fb4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
